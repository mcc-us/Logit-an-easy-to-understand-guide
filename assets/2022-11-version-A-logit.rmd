# Using Logistic Regression for Binary Classification

<center>**With Friendly Math For Beginners**</center>

```{r include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

## Introduction

For scientists who have studied cell biology or biochemistry, logistic regression (a.k.a. logit) may be familiar as [dose-response curves](https://en.wikipedia.org/wiki/Dose%E2%80%93response_relationship), [median lethal dose curves](https://en.wikipedia.org/wiki/Median_lethal_dose) (LD-50) or even exponential growth curves given limited resources. The last example was made famous by Thomas Malthus and his theory of [Malthusian catastrophe](https://en.wikipedia.org/wiki/Malthusianism). 

>A Note on Malthus
>
>[Malthus](https://en.wikipedia.org/wiki/Malthusian_growth_model) believed that humans would eventually reproduce in greater numbers (growing exponentialy) than the ability of human agriculture (growing linearly) to keep up and produce food. This would eventually cause a collapse of the population. It is also known as a Malthusian catastrophe.

If we look at the projected worlds human population from 1950 to 2100, we see its growth and limits are described by the logistic curve.

![](world-pop-1950-2100.jpg)

When we look at the projected world population, we can see each continent is added together liked a stacked bar chart only over time. 

This should make sense since each continent has its own growth rate $\beta_{continent}$ and initial starting population $x_{continent}$.


\begin{equation}
ln ~ ( e^x ) =~ \beta_0 + \beta_{Asia} x_{Asia} + \beta_{Africa} x_{Africa} + ... + \beta_{Oceania} x_{Oceania}
\end{equation}


\begin{equation}
\left( \begin{array}{c} Growth~Rate \\ \overline{\beta_{Oceania}} \\ \beta_{S.America} \\ \beta_{N.America} \\ \beta_{Europe} \\ \beta_{Africa} \\ \beta_{Asia}  \end{array} \right)
\left( \begin{array}{c} Population \\ \overline{\beta_{Oceania}} \\ \beta_{S.America} \\ \beta_{N.America} \\ \beta_{Europe} \\ \beta_{Africa} \\ \beta_{Asia}  \end{array} \right)
+  \beta_{initial}
~=~
\left( \begin{array}{c} \beta_{Oceania} \\ \beta_{S.America} \\ \beta_{N.America} \\ \beta_{Europe} \\ \beta_{Africa} \\ \beta_{Asia}  \end{array} \right)
\end{equation}


$$WX+b ~=~ ln \left( {e^{(WX+b)}} \right)$$

Each continent has its own logistic growth curve. And they are added together liked a stacked bar chart only over time.

However data scientists commonly use Logistic Regression as a classifier for one target variable. For example, the target variable may be churn or no-churn, fraud or no-fraud, go/no-go, and the binary values {0,1}.

```{r echo=FALSE, fig.align="center", fig.height=3.5}
x <- seq(-4, 4, 0.05)
Probability <- 1 / (1 + exp(-x))

plot(x, Probability,
type = "l",
main = "A Simple Logistic Model",
ylim = c(-0.3, 1.3))
abline(h = 0, col = "green")
abline(v = 0, col = "blue", lty = 3, lwd = 3)
abline(h = 1, col = "red", lty = 3)
text(-2, 0.48, cex = 1.3, "if x < 0 then y = 0")
text(2, 0.48, cex = 1.3, "if x >= 0 then y = 1")
```

```{r echo=FALSE, fig.align="center", fig.height=3.5}
x  <- seq(-4, 4, 0.05)
y1 <-  1 / (1 + exp(-x))
y2 <- 0*x

# Boundaries
plot(x, y1, type = "l", ylab = "y")
lines(x, y2, col = 2)

# Fillin Boundaries
polygon(c(x, rev(x)), c(y2, rev(y1)), col = "lightgreen", density=10, angle=45)
```



Logit in a general form can be stated as:

\begin{equation}
f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ x < 0 \\ 1 ~~for~~ x \geq 0 \end{matrix} \right.
\end{equation}

\begin{equation}
f(x) = ~~ \left \{ \begin{matrix} 0 ~~for~~ x < 0 \\ 1 ~~for~~ x \geq 0 \end{matrix} \right.
\end{equation}

Despite its sleek curvature, the [logistic regression](https://en.wikipedia.org/wiki/Logistic_function) is really a linear model.

\begin{equation}
f(x) ~=~ \frac{1}{1 + e^{-(WX+b)}}
\end{equation}

Linear? you may wonder.    
Yes, remember the natural log is the inverse of the exponential function e? What we find is that this transformation of exponential growth drops out and we are left with straight line.

$$ln \left( {e^{(WX+b)}} \right) = WX+b$$

We see that $X$ can take on any real value ($X ~ \in ~ \Re$). The inflection point (shown above in Simple Logistic Curve at $X = 0$) can be moved left or right by adding $b$. The slope of the s-shaped at the inflection point can be modified by changing $W$.  While, the output of a Logistic Regression remains between [0, 1], inclusive. 

In the case where we want to find $b$ and work with a large sets of data points (matrices) we must use linear algebra. To find the vector $\vec{b}$ using matrix multiplication we can calculate: 

$$\overrightarrow{W} \overrightarrow{X}= \vec{b}$$

\begin{equation}
\left[ \begin{array}{cols} 1 & 1 & 6\\ 3 & 0 & 1\\  1 & 1 & 4\end{array}\right]
\left[ \begin{array}{cols} x_1 \\ x_2 \\ x_3 \end{array}\right]
\mbox{ = }
\left[ \begin{array}{cols} 1x_1 + 1x_2 + 6x_3 \\ 3x_1 + 0x_2 + 1x_3 \\ 1x_1 + 1x_2 + 4x_3 \end{array}\right]
\mbox{ = }
\left[ \begin{array}{cols} b_1 \\ b_2 \\ b_3 \end{array}\right]
\end{equation}


## Probabilities and Odds

Besides finding the inflection point (in our simple case where $x=0$) and the slope of a sigmoidal curve, what else can logistic regression teach us? 

Above we learned function fits between [0, 1] so shouldn't be true 

$$P_{success} + P_{failure} = 1





Let's go back to our simple diagram. 

```{r echo=FALSE, fig.align="center", fig.height=4.0}
x <- seq(-3, 3, 0.05)
Probability <- 1 / (1 + exp(-x))

plot(x, Probability,
type = "l",
main = "A Simple Logistic Curve",
ylab = "Probability of Success",
ylim = c(-0.3, 1.3))
abline(h = 0, col = "blue")
abline(h = 1, col = "blue")
abline(v = -1.5, col = "red", lwd = 2)
abline(v = 0, col = "red", lwd = 2)
abline(v = 1.5, col = "red", lwd = 2)
text(-1.5, -0.2, cex = 1.25, "p=0.2 & pf=0.8")
text(1.5, 0.4, cex = 1.25, "p=0.8 & pf=0.2")
```




Since the logistic equation is exponential, it is easier to work with the formula in terms of its odds or *log-odds*. Odds are the probabilities of success over failure denoted as $\Large \frac{p}{1-p}$ and more importantly, in this situation, log-odds are $ln \left (\frac{p}{1-p} \right )$.

Simply by using log-odds, logistic regression may be more easily expressed as a set of linear equations in x.[^43] Hence we can now go from linear regression to logistic regression.

[^43]:http://juangabrielgomila.com/en/logistic-regression-derivation/

Step #1:
\begin{equation}
ln ~ \left ( \frac{Pr(y_i ~=~ 1|x_i)}{Pr(y_i ~=~ 0|x_i)} \right ) =~ \beta_0 + \beta_1 x_1 +~ ... ~+ \beta_{n} x_{n}
\end{equation}

Step #2:
Substitute ($p$ for $Pr(y_i ~=~ 1|x_i)$) and ($1-p$ for $Pr(y_i ~=~ 0|x_i)$) and change notation to summation on the right hand side;

\begin{equation}
ln \left( \frac {p}{1-p} \right) =~ \sum_i^{k} \beta_i x_i
\end{equation}

Step #3:
Eliminate the natural log by taking the exponent on both sides;

\begin{equation}
\frac {p}{1-p} =~ exp \left ( \sum_i^{k} \beta_i x_i \right )
\end{equation}

Step #4:
Substitute $u = \sum_i^{k} \beta_i x_i$;

\begin{equation}
\frac {p}{1-p} =~ e^u
\end{equation}

Step #5:
Rearrange to solve for $\large p$;

\begin{equation}
p(u) ~=~ \frac{e^u}{1 + e^u}
\end{equation}

Step #6:
Incidentally, to find the probabilities, take the derivative of both sides using quotient rule;

\begin{equation}
p'(u) ~=~ \frac {(e^u)(1 + e^u) - (e^u)(e^u)}{(1 + e^u)^2}
\end{equation}

Step #7:
Simplify;

\begin{equation}
p'(u) ~=~ \frac {e^u}{(1 + e^u)^2}
\end{equation}

Step #8:
Separate out to produce two fractions;

\begin{equation}
p'(u) ~=~ \left ( \frac {e^u}{1 + e^u} \right ) \cdot \left ( \frac{1}{1 + e^u} \right )
\end{equation}

Step #9:
Substitute our previous success and failure variables back into place;

\begin{equation}
p'(u) ~=~ p(u) \cdot ( 1 - p(u))
\end{equation}

Now we can calculate the probabilities as well as the values for any given x value.

---

